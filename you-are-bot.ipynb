{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98885,"databundleVersionId":11800270,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Preparation","metadata":{}},{"cell_type":"code","source":"!pip install -q pymorphy3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:44:46.252519Z","iopub.execute_input":"2025-05-10T11:44:46.253055Z","iopub.status.idle":"2025-05-10T11:44:50.975863Z","shell.execute_reply.started":"2025-05-10T11:44:46.253022Z","shell.execute_reply":"2025-05-10T11:44:50.975213Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, \\\n    TrainingArguments, Trainer\nfrom xgboost import XGBClassifier\nfrom datasets import Dataset\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport copy\nimport re\nimport string\nfrom pymorphy3 import MorphAnalyzer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WordPunctTokenizer\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:44:50.977500Z","iopub.execute_input":"2025-05-10T11:44:50.977713Z","iopub.status.idle":"2025-05-10T11:45:20.085737Z","shell.execute_reply.started":"2025-05-10T11:44:50.977695Z","shell.execute_reply":"2025-05-10T11:45:20.084930Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 11:45:05.668125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746877505.894957      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746877505.959750      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"BERT_CHECKPOINT = \"ai-forever/ruRoberta-large\"\nPERPLEXITY_MODEL_CHECKPOINT = \"openai-community/gpt2\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTMP_DIR = Path('../temp')\nTMP_DIR.mkdir(exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:20.086560Z","iopub.execute_input":"2025-05-10T11:45:20.087129Z","iopub.status.idle":"2025-05-10T11:45:20.090978Z","shell.execute_reply.started":"2025-05-10T11:45:20.087108Z","shell.execute_reply":"2025-05-10T11:45:20.090301Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\nbert_model = AutoModel.from_pretrained(BERT_CHECKPOINT).to(DEVICE)\n\nppx_tokenizer = AutoTokenizer.from_pretrained(PERPLEXITY_MODEL_CHECKPOINT)\nppx_model = AutoModelForCausalLM.from_pretrained(PERPLEXITY_MODEL_CHECKPOINT).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:20.092803Z","iopub.execute_input":"2025-05-10T11:45:20.093273Z","iopub.status.idle":"2025-05-10T11:45:37.898621Z","shell.execute_reply.started":"2025-05-10T11:45:20.093244Z","shell.execute_reply":"2025-05-10T11:45:37.897813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba66fc8ae304c74807fa560b8a8325c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af41ec3072ca4b03a2370e7991c45c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f55f924dd74c94a731fe30f9f4cea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd89f4f8bb9f4f66ac190f98d2c11bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7bd28b881e4e74a40449c0ee102ffa"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89846e2a89c142cda2c3becc2c376f19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e89d749f76543f09f4a76b71fb41966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451aefad602e45d693240f96e29d80cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00bd7e090cd44abdaca5211e770ae830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefc2a085d154dc19ad83311d08fb367"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da7f889069e4a9b9d69113bfb9dd2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0985bb6d4a4c4f9990c0771b01d248f4"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def load_train_data(data_file: str, labels_file: str):\n    all_texts = []\n    all_labels = []\n\n    labels_df = pd.read_csv(labels_file)\n    labels_df = labels_df[labels_df[\"participant_index\"] == 0]\n    labels_dict = dict(zip(labels_df[\"dialog_id\"], labels_df[\"is_bot\"]))\n\n    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n\n        data = json.load(f)\n        for key in data.keys():\n            messages = data[key]\n\n            part_0_texts = [\n                m[\"text\"] for m in messages if m[\"participant_index\"] == \"0\"\n            ]\n            part_1_texts = [\n                m[\"text\"] for m in messages if m[\"participant_index\"] == \"1\"\n            ]\n\n            part_0_label = int(labels_dict[key])\n            part_1_label = 1 - part_0_label\n\n            text_0 = \" \".join(part_0_texts)\n            text_1 = \" \".join(part_1_texts)\n\n            all_texts.append(text_0)\n            all_labels.append(part_0_label)\n\n            all_texts.append(text_1)\n            all_labels.append(part_1_label)\n\n    df = pd.DataFrame({\"text\": all_texts, \"is_bot\": all_labels})\n    return df\n\ndef load_test_data(data_file: str, labels_file: str):\n    df_info = pd.read_csv(labels_file)\n\n    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    all_texts = []\n    ids = []\n\n    for _, row in df_info.iterrows():\n        dialog_id = row[\"dialog_id\"]\n        participant_index = str(row[\"participant_index\"])\n        messages = data[dialog_id]\n\n        texts = [\n            m[\"text\"] for m in messages if m[\"participant_index\"] == participant_index\n        ]\n        combined_text = \" \".join(texts)\n        all_texts.append(combined_text)\n        ids.append(row[\"ID\"])\n\n    df = pd.DataFrame({\"ID\": ids, \"text\": all_texts})\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:37.899659Z","iopub.execute_input":"2025-05-10T11:45:37.899930Z","iopub.status.idle":"2025-05-10T11:45:37.911668Z","shell.execute_reply.started":"2025-05-10T11:45:37.899909Z","shell.execute_reply":"2025-05-10T11:45:37.910964Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def save_submission(name, preds, sample_submission):\n    submission = copy.deepcopy(sample_submission)\n    submission[\"is_bot\"] = preds\n    submission.to_csv(f\"/kaggle/working/{name}.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:37.912732Z","iopub.execute_input":"2025-05-10T11:45:37.913046Z","iopub.status.idle":"2025-05-10T11:45:42.037172Z","shell.execute_reply.started":"2025-05-10T11:45:37.913017Z","shell.execute_reply":"2025-05-10T11:45:42.036346Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Bert functions","metadata":{}},{"cell_type":"code","source":"class CustomRoberta(nn.Module):\n    def __init__(self, base_model, num_classes=2, dropout_rate=0.1):\n        super().__init__()\n        self.base_model = base_model\n        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_classes)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        output = self.base_model(input_ids, attention_mask = attention_mask)\n        pooled_output = output.last_hidden_state[:, 0, :]\n        logits = self.classifier(self.dropout(pooled_output))\n\n        loss = None\n        if labels is not None:\n            loss_func = nn.CrossEntropyLoss()\n            loss = loss_func(logits, labels)\n            \n        return {\"loss\": loss, \"logits\": logits}\n\ndef tokenize(examples, tokenizer=bert_tokenizer):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:42.038236Z","iopub.execute_input":"2025-05-10T11:45:42.038458Z","iopub.status.idle":"2025-05-10T11:45:42.977153Z","shell.execute_reply.started":"2025-05-10T11:45:42.038436Z","shell.execute_reply":"2025-05-10T11:45:42.976331Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_metrics_bert(dataset, model):\n    model.eval()\n    torch_dataset = dataset.with_format(\"torch\")\n    dataloader = DataLoader(torch_dataset, batch_size=32)\n\n    true_labels = []\n    pred_labels = []\n    losses = []\n    with torch.no_grad():   \n        for batch in tqdm(dataloader):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = batch[\"labels\"].to(DEVICE)\n            output = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss, logits = output[\"loss\"], output[\"logits\"]\n            _, preds = torch.max(logits.cpu(), 1)\n            \n            true_labels.extend(labels.cpu().numpy())\n            pred_labels.extend(preds.cpu().numpy())\n            losses.append(loss.cpu())\n    \n    model.train()\n    metrics = {\n        \"accuracy\": accuracy_score(true_labels, pred_labels),\n        \"f1_score\": f1_score(true_labels, pred_labels),\n        \"loss\": np.mean(losses, axis=0)\n    }\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:42.978072Z","iopub.execute_input":"2025-05-10T11:45:42.978347Z","iopub.status.idle":"2025-05-10T11:45:44.141769Z","shell.execute_reply.started":"2025-05-10T11:45:42.978311Z","shell.execute_reply":"2025-05-10T11:45:44.140889Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def make_preds_bert(dataset, model):\n    model.eval()\n    torch_dataset = dataset.with_format(\"torch\")\n    dataloader = DataLoader(torch_dataset, batch_size=32)\n\n    preds = []\n    with torch.no_grad():   \n        for batch in tqdm(dataloader):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            output = model(input_ids, attention_mask=attention_mask)\n            logits = output[\"logits\"]\n\n            pred_probs = torch.softmax(logits, dim=1)[:, 1]\n            pred_probs = [p.item() for p in pred_probs]\n            preds.extend(pred_probs)\n    \n    model.train()\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:44.142644Z","iopub.execute_input":"2025-05-10T11:45:44.142872Z","iopub.status.idle":"2025-05-10T11:45:44.933586Z","shell.execute_reply.started":"2025-05-10T11:45:44.142854Z","shell.execute_reply":"2025-05-10T11:45:44.932757Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#### Perplexity based model functions","metadata":{}},{"cell_type":"code","source":"def calculate_perplexity(text, model, tokenizer):\n    \n    encodings = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n    encodings = {k: v.to(DEVICE) for k, v in encodings.items()}\n\n    input_ids = encodings[\"input_ids\"]\n    with torch.no_grad():\n        output = model(**encodings, labels=input_ids)\n        nll_loss = output[\"loss\"].item()\n\n    return np.exp(nll_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:44.937647Z","iopub.execute_input":"2025-05-10T11:45:44.937877Z","iopub.status.idle":"2025-05-10T11:45:45.106039Z","shell.execute_reply.started":"2025-05-10T11:45:44.937858Z","shell.execute_reply":"2025-05-10T11:45:45.105450Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def predict_with_perplexity(perplexity, threshold):\n    if perplexity >= threshold:\n        difference = perplexity - threshold\n        \n        if difference > 1000:\n            return (0.8, 0.2)\n        elif difference > 500:\n            return (0.65, 0.35)\n        elif difference > 200:\n            return (0.6, 0.4)\n        return (0.55, 0.45)\n\n    difference = threshold - perplexity\n        \n    if difference > 1000:\n        return (0.2, 0.8)\n    elif difference > 500:\n        return (0.35, 0.65)\n    elif difference > 200:\n        return (0.4, 0.6)\n    return (0.45, 0.55)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.106731Z","iopub.execute_input":"2025-05-10T11:45:45.106977Z","iopub.status.idle":"2025-05-10T11:45:45.121743Z","shell.execute_reply.started":"2025-05-10T11:45:45.106950Z","shell.execute_reply":"2025-05-10T11:45:45.121144Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def compute_metrics_ppx(dataframe, logits):\n    true_labels = dataframe[\"is_bot\"].values\n    _, pred_labels = torch.max(logits.cpu(), 1)\n    \n    loss_func = nn.CrossEntropyLoss()\n    loss = loss_func(logits, torch.tensor(true_labels))\n    \n    metrics = {\n        \"accuracy\": accuracy_score(true_labels, pred_labels),\n        \"f1_score\": f1_score(true_labels, pred_labels),\n        \"loss\": loss\n    }\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.122334Z","iopub.execute_input":"2025-05-10T11:45:45.122567Z","iopub.status.idle":"2025-05-10T11:45:45.137758Z","shell.execute_reply.started":"2025-05-10T11:45:45.122547Z","shell.execute_reply":"2025-05-10T11:45:45.137244Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"#### Functions for logreg/xgbclassifier model","metadata":{}},{"cell_type":"code","source":"def extract_features_from_df(df_to_process, train_df):\n    new_df = copy.deepcopy(df_to_process)\n    \n    new_df[\"char_len\"] = new_df[\"text\"].apply(len)\n    new_df[\"word_len\"] = new_df[\"text\"].apply(lambda x: len(WordPunctTokenizer().tokenize(x)))\n    \n    emoji_pattern = re.compile(r'[:;][-]?[\\)\\(DP]|[\\U0001F600-\\U0001F64F]')\n    new_df['has_emoji'] = new_df['text'].apply(lambda x: 1 if emoji_pattern.search(x) else 0)\n    new_df[\"qu_ex_marks\"] = new_df[\"text\"].apply(lambda x: x.count(\"!\") + x.count(\"?\"))\n    new_df[\"punct_count\"] = new_df[\"text\"].apply(\n        lambda x: sum([1 for ch in x if ch in string.punctuation])\n    )\n    new_df[\"upper_char_ratio\"] = new_df[\"text\"].apply(\n        lambda x: sum([1 for ch in x if ch.isupper()]) / max(1, len(x))\n    )\n\n    morph = MorphAnalyzer()\n    def lemmatize(text):\n        total = []\n        words = WordPunctTokenizer().tokenize(text.lower())\n        for word in words:\n            total.append(morph.parse(word)[0].normal_form)\n        return \" \".join(total)\n        \n    new_df[\"lemmatized\"] = new_df[\"text\"].apply(lemmatize)\n\n    new_df[\"unique_word_ratio\"] = new_df[\"lemmatized\"].apply(\n        lambda x: len(set(x.split())) / max(1, len(x.split()))\n    )\n    \n    stopwords_set = set(stopwords.words(\"russian\"))\n    stopwords_set.update(stopwords.words(\"english\"))\n    new_df[\"stopword_ratio\"] = new_df[\"lemmatized\"].apply(\n        lambda x: len([w for w in x.split() if w in stopwords_set]) / max(1, len(x.split()))\n    )\n\n    train_df_with_lemmatized = copy.deepcopy(train_df)\n    train_df_with_lemmatized[\"lemmatized\"] = train_df_with_lemmatized[\"text\"].apply(lemmatize)\n    all_words = [\n        word.lower()\n        for text in train_df_with_lemmatized[\"lemmatized\"]\n        for word in WordPunctTokenizer().tokenize(text.lower())\n    ]\n    word_freq = Counter(all_words)\n    top_100_words = set(word for word, _ in word_freq.most_common(100))\n    new_df[\"rare_word_freq\"] = new_df[\"lemmatized\"].apply(\n        lambda x: len([w for w in x.split() if w not in top_100_words]) / max(1, len(x.split()))\n    )\n\n    tfidf = TfidfVectorizer(max_features=1000)\n    tfidf = tfidf.fit(train_df_with_lemmatized[\"lemmatized\"])\n    sparse_matrix = tfidf.transform(new_df[\"lemmatized\"])\n    new_df[\"tf_idf_mean\"] = sparse_matrix.sum(axis=1) / sparse_matrix.shape[1]\n\n    return new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.138500Z","iopub.execute_input":"2025-05-10T11:45:45.138738Z","iopub.status.idle":"2025-05-10T11:45:45.160097Z","shell.execute_reply.started":"2025-05-10T11:45:45.138714Z","shell.execute_reply":"2025-05-10T11:45:45.159394Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def compute_metrics_logreg(X, y, model):\n    logits = torch.tensor(model.predict_proba(X))\n    _, pred_labels = torch.max(logits, axis=1)\n\n    loss_func = nn.CrossEntropyLoss()\n    loss = loss_func(logits, torch.tensor(y.values))\n    \n    metrics = {\n        \"accuracy\": accuracy_score(y.values, pred_labels),\n        \"f1_score\": f1_score(y.values, pred_labels),\n        \"loss\": loss\n    }\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.160650Z","iopub.execute_input":"2025-05-10T11:45:45.160871Z","iopub.status.idle":"2025-05-10T11:45:45.178505Z","shell.execute_reply.started":"2025-05-10T11:45:45.160845Z","shell.execute_reply":"2025-05-10T11:45:45.178034Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def make_preds_logreg(X, model):\n    preds = xgb_model.predict_proba(X)[:,1]\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.179227Z","iopub.execute_input":"2025-05-10T11:45:45.179464Z","iopub.status.idle":"2025-05-10T11:45:45.194659Z","shell.execute_reply.started":"2025-05-10T11:45:45.179443Z","shell.execute_reply":"2025-05-10T11:45:45.194173Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"#### Ensemble of models (Weighted Averaging) functions","metadata":{}},{"cell_type":"code","source":"def predict_with_3_models(bert, ppx_threshold, ppx_model, ppx_tokenizer,\n                          logreg, test_df, test_dataset, train_df):    \n    bert_preds = make_preds_bert(test_dataset, bert)\n    \n    ppx_preds = []\n    test_ppx = [] # Сохраним значения perplexity для след. модели\n    for i in test_data.index:\n        text = test_data.iloc[i][\"text\"]\n        ppx = calculate_perplexity(text, ppx_model, ppx_tokenizer)\n        test_ppx.append(ppx)\n        ppx_preds.append(predict_with_perplexity(ppx, ppx_threshold)[1])\n        \n    test_df_with_perplexity = copy.deepcopy(test_df)\n    test_df_with_perplexity[\"perplexity\"] = test_ppx\n\n    new_test_data = extract_features_from_df(test_df_with_perplexity, train_df)\n    X_test = new_test_data.drop(columns=[\"ID\", \"text\", \"lemmatized\"])\n    xgb_preds = make_preds_logreg(X_test, xgb_model)\n    \n    ensemble_preds = []\n    for i in range(len(bert_preds)):\n        ensemble_pred = bert_preds[i] * 0.7 + ppx_preds[i] * 0.1 + xgb_preds[i] * 0.2\n        ensemble_preds.append(ensemble_pred)\n\n    return ensemble_preds    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.195424Z","iopub.execute_input":"2025-05-10T11:45:45.195635Z","iopub.status.idle":"2025-05-10T11:45:45.210855Z","shell.execute_reply.started":"2025-05-10T11:45:45.195614Z","shell.execute_reply":"2025-05-10T11:45:45.210298Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def predict_with_2_models(bert, logreg, test_df, test_dataset, train_df):    \n    bert_preds = make_preds_bert(test_dataset, bert)\n    \n    test_ppx = [] # Сохраним значения perplexity для след. модели\n    for i in test_data.index:\n        text = test_data.iloc[i][\"text\"]\n        ppx = calculate_perplexity(text, ppx_model, ppx_tokenizer)\n        test_ppx.append(ppx)\n        \n    test_df_with_perplexity = copy.deepcopy(test_df)\n    test_df_with_perplexity[\"perplexity\"] = test_ppx\n\n    new_test_data = extract_features_from_df(test_df_with_perplexity, train_df)\n    X_test = new_test_data.drop(columns=[\"ID\", \"text\", \"lemmatized\"])\n    xgb_preds = make_preds_logreg(X_test, xgb_model)\n    \n    ensemble_preds = []\n    for i in range(len(bert_preds)):\n        ensemble_pred = bert_preds[i] * 0.6 + xgb_preds[i] * 0.4\n        ensemble_preds.append(ensemble_pred)\n\n    return ensemble_preds    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:16:13.319167Z","iopub.execute_input":"2025-05-10T12:16:13.319441Z","iopub.status.idle":"2025-05-10T12:16:13.325129Z","shell.execute_reply.started":"2025-05-10T12:16:13.319425Z","shell.execute_reply":"2025-05-10T12:16:13.324396Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"#### Data load","metadata":{}},{"cell_type":"code","source":"train_data = load_train_data(\"/kaggle/input/you-are-bot/train.json\", \"/kaggle/input/you-are-bot/ytrain.csv\")\ntest_data = load_test_data(\"/kaggle/input/you-are-bot/test.json\", \"/kaggle/input/you-are-bot/ytest.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/you-are-bot/sample_submission.csv\", index_col=\"ID\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.231544Z","iopub.execute_input":"2025-05-10T11:45:45.231717Z","iopub.status.idle":"2025-05-10T11:45:45.360960Z","shell.execute_reply.started":"2025-05-10T11:45:45.231703Z","shell.execute_reply":"2025-05-10T11:45:45.360442Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"sns.barplot(x = train_data[\"is_bot\"].unique(), y = train_data[\"is_bot\"].value_counts())\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.361694Z","iopub.execute_input":"2025-05-10T11:45:45.361932Z","iopub.status.idle":"2025-05-10T11:45:45.535686Z","shell.execute_reply.started":"2025-05-10T11:45:45.361910Z","shell.execute_reply":"2025-05-10T11:45:45.535129Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"train_df, val_df = train_test_split(train_data, test_size=0.15)\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Убираем ненужную колонку\ntrain_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\nval_dataset = val_dataset.remove_columns([\"__index_level_0__\"])\n\n# Переименовываем таргетную колонку\ntrain_dataset = train_dataset.rename_column(\"is_bot\", \"labels\")\nval_dataset = val_dataset.rename_column(\"is_bot\", \"labels\")\n\n# Токенизируем датасет\ntrain_dataset = train_dataset.map(tokenize, batched=True)\nval_dataset = val_dataset.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:45.536300Z","iopub.execute_input":"2025-05-10T11:45:45.536555Z","iopub.status.idle":"2025-05-10T11:45:46.079074Z","shell.execute_reply.started":"2025-05-10T11:45:45.536532Z","shell.execute_reply":"2025-05-10T11:45:46.078347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1336 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5279b1fa1e4c1ca14773d844ab0a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/236 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6bfafa7c1242bab9157aa87820149f"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"test_dataset = Dataset.from_pandas(test_data)\ntest_dataset = test_dataset.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:46.079921Z","iopub.execute_input":"2025-05-10T11:45:46.080129Z","iopub.status.idle":"2025-05-10T11:45:46.291382Z","shell.execute_reply.started":"2025-05-10T11:45:46.080112Z","shell.execute_reply":"2025-05-10T11:45:46.290765Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/676 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d44b2a6fd74d7ab453c23ade84c0e1"}},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"### BERT MODEL","metadata":{}},{"cell_type":"code","source":"roberta = CustomRoberta(bert_model)\nroberta = roberta.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:46.292120Z","iopub.execute_input":"2025-05-10T11:45:46.292386Z","iopub.status.idle":"2025-05-10T11:45:46.301038Z","shell.execute_reply.started":"2025-05-10T11:45:46.292362Z","shell.execute_reply":"2025-05-10T11:45:46.300484Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir= TMP_DIR,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=2e-5,\n    logging_strategy=\"epoch\", \n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    greater_is_better=False,       \n    report_to=\"none\",\n    load_best_model_at_end=True,\n    fp16=True\n)\n\ntrainer = Trainer(\n    model=roberta,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:46.314920Z","iopub.execute_input":"2025-05-10T11:45:46.315132Z","iopub.status.idle":"2025-05-10T11:45:46.374681Z","shell.execute_reply.started":"2025-05-10T11:45:46.315116Z","shell.execute_reply":"2025-05-10T11:45:46.373955Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:45:46.375435Z","iopub.execute_input":"2025-05-10T11:45:46.375666Z","iopub.status.idle":"2025-05-10T12:06:16.503317Z","shell.execute_reply.started":"2025-05-10T11:45:46.375644Z","shell.execute_reply":"2025-05-10T12:06:16.502555Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [420/420 20:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.728700</td>\n      <td>0.654030</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.652600</td>\n      <td>0.636998</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.553600</td>\n      <td>0.687069</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.488500</td>\n      <td>0.675319</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.411100</td>\n      <td>0.677855</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=420, training_loss=0.5669084912254697, metrics={'train_runtime': 1229.6658, 'train_samples_per_second': 5.432, 'train_steps_per_second': 0.342, 'total_flos': 0.0, 'train_loss': 0.5669084912254697, 'epoch': 5.0})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(compute_metrics_bert(val_dataset, roberta))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:06:16.504252Z","iopub.execute_input":"2025-05-10T12:06:16.504546Z","iopub.status.idle":"2025-05-10T12:06:39.409265Z","shell.execute_reply.started":"2025-05-10T12:06:16.504522Z","shell.execute_reply":"2025-05-10T12:06:39.408500Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 8/8 [00:22<00:00,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"{'accuracy': 0.6228813559322034, 'f1_score': 0.642570281124498, 'loss': 0.6309968}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"bert_preds = make_preds_bert(test_dataset, roberta)\nsave_submission(\"submission_bert\", bert_preds, sample_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:06:39.410133Z","iopub.execute_input":"2025-05-10T12:06:39.410363Z","iopub.status.idle":"2025-05-10T12:07:45.004868Z","shell.execute_reply.started":"2025-05-10T12:06:39.410346Z","shell.execute_reply":"2025-05-10T12:07:45.004220Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 22/22 [01:05<00:00,  2.98s/it]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### PERPLEXITY BASED MODEL","metadata":{}},{"cell_type":"code","source":"perplexities = []\nfor i in tqdm(train_df.index):\n    text = train_df.loc[i][\"text\"]\n    ppx = calculate_perplexity(text, ppx_model, ppx_tokenizer)\n    perplexities.append(ppx)\n\ntrain_df[\"perplexity\"] = perplexities\n\nperplexities = []\nfor i in tqdm(val_df.index):\n    text = val_df.loc[i][\"text\"]\n    ppx = calculate_perplexity(text, ppx_model, ppx_tokenizer)\n    perplexities.append(ppx)\n\nval_df[\"perplexity\"] = perplexities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:07:45.008442Z","iopub.execute_input":"2025-05-10T12:07:45.008659Z","iopub.status.idle":"2025-05-10T12:08:05.683176Z","shell.execute_reply.started":"2025-05-10T12:07:45.008642Z","shell.execute_reply":"2025-05-10T12:08:05.682348Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1336 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n100%|██████████| 1336/1336 [00:17<00:00, 76.06it/s]\n100%|██████████| 236/236 [00:03<00:00, 76.19it/s]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"perplexities = train_df[\"perplexity\"].values\nlabels = train_df[\"is_bot\"].values\n\nthresholds = np.sort(perplexities)\n\nbest_thershold = None\nbest_f1 = 0\n\nfor threshold in thresholds:\n    preds = (perplexities < threshold).astype(int)\n    f1 = f1_score(labels, preds)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_threshold = threshold\n\nprint(f\"f1_score for train data: {best_f1}, threshold: {best_threshold}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:05.684104Z","iopub.execute_input":"2025-05-10T12:08:05.684382Z","iopub.status.idle":"2025-05-10T12:08:07.302345Z","shell.execute_reply.started":"2025-05-10T12:08:05.684358Z","shell.execute_reply":"2025-05-10T12:08:07.301588Z"}},"outputs":[{"name":"stdout","text":"f1_score for train data: 0.664321608040201, threshold: 1915.7841755787535\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"logits = []\nfor i in val_df.index:\n    cur_logit = predict_with_perplexity(val_df.loc[i][\"perplexity\"], best_threshold)\n    logits.append(cur_logit)\n\nprint(compute_metrics_ppx(val_df, torch.tensor(logits)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:07.303151Z","iopub.execute_input":"2025-05-10T12:08:07.303423Z","iopub.status.idle":"2025-05-10T12:08:07.368768Z","shell.execute_reply.started":"2025-05-10T12:08:07.303396Z","shell.execute_reply":"2025-05-10T12:08:07.368021Z"}},"outputs":[{"name":"stdout","text":"{'accuracy': 0.5169491525423728, 'f1_score': 0.6779661016949153, 'loss': tensor(0.7299)}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"ppx_preds = []\ntest_ppx = [] # Сохраним значения perplexity для след. модели\n\nfor i in test_data.index:\n    text = test_data.iloc[i][\"text\"]\n    ppx = calculate_perplexity(text, ppx_model, ppx_tokenizer)\n    test_ppx.append(ppx)\n    ppx_preds.append(predict_with_perplexity(ppx, best_threshold)[1])\n\ntest_data[\"perplexity\"] = test_ppx\n\nsave_submission(\"submission_ppx\", ppx_preds, sample_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:15.554721Z","iopub.execute_input":"2025-05-10T12:09:15.555602Z","iopub.status.idle":"2025-05-10T12:09:24.383239Z","shell.execute_reply.started":"2025-05-10T12:09:15.555577Z","shell.execute_reply":"2025-05-10T12:09:24.382427Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### LogisticRegression and XGBClassifier on features","metadata":{}},{"cell_type":"code","source":"new_train_df = extract_features_from_df(train_df, train_df)\nnew_val_df = extract_features_from_df(val_df, train_df)\n\nX_train = new_train_df.drop(columns=[\"text\", \"is_bot\", \"lemmatized\"])\ny_train = new_train_df[\"is_bot\"]\n\nX_val = new_val_df.drop(columns=[\"text\", \"is_bot\", \"lemmatized\"])\ny_val = new_val_df[\"is_bot\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:24.384529Z","iopub.execute_input":"2025-05-10T12:09:24.384739Z","iopub.status.idle":"2025-05-10T12:09:32.217038Z","shell.execute_reply.started":"2025-05-10T12:09:24.384721Z","shell.execute_reply":"2025-05-10T12:09:32.216282Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"log_reg_model = LogisticRegression().fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:32.217841Z","iopub.execute_input":"2025-05-10T12:09:32.218076Z","iopub.status.idle":"2025-05-10T12:09:32.616368Z","shell.execute_reply.started":"2025-05-10T12:09:32.218057Z","shell.execute_reply":"2025-05-10T12:09:32.613452Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(compute_metrics_logreg(X_val, y_val, log_reg_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:32.617587Z","iopub.execute_input":"2025-05-10T12:09:32.621139Z","iopub.status.idle":"2025-05-10T12:09:32.653297Z","shell.execute_reply.started":"2025-05-10T12:09:32.621119Z","shell.execute_reply":"2025-05-10T12:09:32.652699Z"}},"outputs":[{"name":"stdout","text":"{'accuracy': 0.5889830508474576, 'f1_score': 0.5125628140703516, 'loss': tensor(0.6833, dtype=torch.float64)}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"xgb_model = XGBClassifier(\n    n_estimators=100,  \n    max_depth=3,       \n    learning_rate=0.1, \n    eval_metric=\"logloss\", \n    early_stopping_rounds=10,  \n    use_label_encoder=False,\n).fit(X_train, y_train, eval_set=[(X_val, y_val)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:32.654676Z","iopub.execute_input":"2025-05-10T12:09:32.655154Z","iopub.status.idle":"2025-05-10T12:09:32.848171Z","shell.execute_reply.started":"2025-05-10T12:09:32.655134Z","shell.execute_reply":"2025-05-10T12:09:32.847636Z"}},"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-logloss:0.68714\n[1]\tvalidation_0-logloss:0.68178\n[2]\tvalidation_0-logloss:0.67795\n[3]\tvalidation_0-logloss:0.67586\n[4]\tvalidation_0-logloss:0.67297\n[5]\tvalidation_0-logloss:0.66993\n[6]\tvalidation_0-logloss:0.66842\n[7]\tvalidation_0-logloss:0.66642\n[8]\tvalidation_0-logloss:0.66568\n[9]\tvalidation_0-logloss:0.66405\n[10]\tvalidation_0-logloss:0.66381\n[11]\tvalidation_0-logloss:0.66353\n[12]\tvalidation_0-logloss:0.66322\n[13]\tvalidation_0-logloss:0.66356\n[14]\tvalidation_0-logloss:0.66334\n[15]\tvalidation_0-logloss:0.66089\n[16]\tvalidation_0-logloss:0.66100\n[17]\tvalidation_0-logloss:0.65745\n[18]\tvalidation_0-logloss:0.65904\n[19]\tvalidation_0-logloss:0.65786\n[20]\tvalidation_0-logloss:0.65756\n[21]\tvalidation_0-logloss:0.65816\n[22]\tvalidation_0-logloss:0.65639\n[23]\tvalidation_0-logloss:0.65746\n[24]\tvalidation_0-logloss:0.65923\n[25]\tvalidation_0-logloss:0.66079\n[26]\tvalidation_0-logloss:0.66090\n[27]\tvalidation_0-logloss:0.66257\n[28]\tvalidation_0-logloss:0.66392\n[29]\tvalidation_0-logloss:0.66329\n[30]\tvalidation_0-logloss:0.66410\n[31]\tvalidation_0-logloss:0.66312\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"print(compute_metrics_logreg(X_val, y_val, xgb_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:32.849031Z","iopub.execute_input":"2025-05-10T12:09:32.849218Z","iopub.status.idle":"2025-05-10T12:09:32.858137Z","shell.execute_reply.started":"2025-05-10T12:09:32.849202Z","shell.execute_reply":"2025-05-10T12:09:32.857419Z"}},"outputs":[{"name":"stdout","text":"{'accuracy': 0.614406779661017, 'f1_score': 0.5284974093264247, 'loss': tensor(0.6659)}\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"XGBClassifier показал лучше качество, чем LogisticRegression, поэтому будем использовать его","metadata":{}},{"cell_type":"code","source":"new_test_data = extract_features_from_df(test_data, train_df)\nX_test = new_test_data.drop(columns=[\"ID\", \"text\", \"lemmatized\"])\n\nxgb_preds = make_preds_logreg(X_test, xgb_model)\nsave_submission(\"submission_xgb\", xgb_preds, sample_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:32.858759Z","iopub.execute_input":"2025-05-10T12:09:32.858945Z","iopub.status.idle":"2025-05-10T12:09:36.313733Z","shell.execute_reply.started":"2025-05-10T12:09:32.858930Z","shell.execute_reply":"2025-05-10T12:09:36.313139Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"### Ensemble of models (Weighted Averaging)","metadata":{}},{"cell_type":"code","source":"ensemble_3_models_preds = predict_with_3_models(roberta, best_threshold, ppx_model,\n                                                ppx_tokenizer, xgb_model,\n                                                test_data, test_dataset, train_df)\nsave_submission(\"submission_ensemble_3_models_preds\",\n                ensemble_3_models_preds, sample_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:09:36.314350Z","iopub.execute_input":"2025-05-10T12:09:36.314540Z","iopub.status.idle":"2025-05-10T12:10:54.626881Z","shell.execute_reply.started":"2025-05-10T12:09:36.314523Z","shell.execute_reply":"2025-05-10T12:10:54.626300Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 22/22 [01:06<00:00,  3.00s/it]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"ensemble_2_models_preds = predict_with_2_models(roberta, xgb_model,\n                                                test_data, test_dataset, train_df)\nsave_submission(\"submission_ensemble_2_models_preds\",\n                ensemble_2_models_preds, sample_submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:16:19.091572Z","iopub.execute_input":"2025-05-10T12:16:19.092282Z","iopub.status.idle":"2025-05-10T12:17:37.340875Z","shell.execute_reply.started":"2025-05-10T12:16:19.092253Z","shell.execute_reply":"2025-05-10T12:17:37.340120Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 22/22 [01:05<00:00,  2.99s/it]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"В ходе работы было протестировано 5 подходов: fine-tuning ruRoberta, perplexity-based подход, XGBclassifier на вытащенных фичах, а также ансамбль из этих 3 моделей и ансамбль из ruRoberta и XGBClassifier\n\n- ruRoberta: самый стандартный подход, дообучить BERT. Получился неплохой, но и не хороший результат. Качество лучше бэйзлайна, но не намного\n- perplexity-based: пороговый детектор на основе перплексии, которую считала модель GPT2 оказался худшим подходом из всех. Это неудивительно, потому что данные специфичные, и бот выдает много странных вещей. Следовательно, самое низкое качество\n- XGBClassifier: немного обогнал ruRoberta. С помощью вытащенных фичей модель научилась выделять закономерности, но, опять же, качество не самое великолепное\n- Ансамбль из 3 моделей: добился такого же результата, как и чистый XGBClassifier. Но, учитывая, насколько бесполезный пороговый детектор, смысла от этой модели в ансамбле мало\n- Ансамбль из ruRoberta и XGBClassifier: лучшее качество из всех опробованных подходов. Путем взвешенного усреднения (предсказание BERT * 0.6 + предсказание XGBClassifier * 0.4) ансамбль моделей смог учитывай большее количество информации, чем модели по отдельности. LogLoss на тестовой выборке = 0.587","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}